{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d9a5a98",
   "metadata": {},
   "source": [
    "# 1. What is the difference between a list and a tuple in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53109a7b",
   "metadata": {},
   "source": [
    "The key differences between tuples and lists are,  \n",
    "1. Tuples are immutable objects, which means once they are created, their elements cannot be changed or modified. You cannot add or remove elements from a tuple. \n",
    "Lists are mutable, meaning you can add, remove, or modify elements after the list is created. You can use methods like append(), extend(), remove(), and pop() to modify a list in-place.\n",
    "2. Tuples are more memory efficient than the lists. When it comes to time efficiency, tuples have a slight advantage over lists.\n",
    "3. List: Define using square brackets [], where Tuple: Define using parentheses ().\n",
    "4. Use cases are, \n",
    "List: Use lists when thers is a need of collection that can be modified, such as when you want to add or remove elements.\n",
    "Tuple: Use tuples when we want an immutable, ordered collection, especially for situations where the data should not be changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54301830",
   "metadata": {},
   "source": [
    "# 2. How can you iterate through a list in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05037d20",
   "metadata": {},
   "source": [
    "A list can be iterated using many methods like using for loop, while loop, range and enumerate. Examples are,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f5686ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method 1: Using a for Loop\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "\n",
      "Method 2: Using range and Indexing\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "\n",
      "Method 3: Using enumerate\n",
      "Index: 0, Value: 1\n",
      "Index: 1, Value: 2\n",
      "Index: 2, Value: 3\n",
      "Index: 3, Value: 4\n",
      "Index: 4, Value: 5\n",
      "\n",
      "Method 4: Using a while Loop\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for loop\n",
    "l1 = [1, 2, 3, 4, 5]\n",
    "print(\"Method 1: Using a for Loop\")\n",
    "for item in l1:\n",
    "    print(item)\n",
    "print()\n",
    "\n",
    "# Using range\n",
    "l2 = [1, 2, 3, 4, 5]\n",
    "print(\"Method 2: Using range and Indexing\")\n",
    "for i in range(len(l2)):\n",
    "    print(l2[i])\n",
    "print()\n",
    "\n",
    "# Using enumerate\n",
    "l3 = [1, 2, 3, 4, 5]\n",
    "print(\"Method 3: Using enumerate\")\n",
    "for index, value in enumerate(l3):\n",
    "    print(f\"Index: {index}, Value: {value}\")\n",
    "print()\n",
    "\n",
    "# while Loop\n",
    "l4 = [1, 2, 3, 4, 5]\n",
    "print(\"Method 4: Using a while Loop\")\n",
    "index = 0\n",
    "while index < len(l4):\n",
    "    print(l4[index])\n",
    "    index += 1\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f962525",
   "metadata": {},
   "source": [
    "# 3. How do you handle exceptions in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce0216",
   "metadata": {},
   "source": [
    "The try...except block is used to handle exceptions in Python. Here's the syntax of try...except block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4aa17ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "    # code that may cause exception\n",
    "# except:\n",
    "    # code to run when exception occurs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a163cd",
   "metadata": {},
   "source": [
    "Here, we have placed the code that might generate an exception inside the try block. Every try block is followed by an except block.\n",
    "When an exception occurs, it is caught by the except block. The except block cannot be used without the try block. Example below,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "86ca2894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Denominator cannot be 0.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    numerator = 10\n",
    "    denominator = 0\n",
    "\n",
    "    result = numerator/denominator\n",
    "\n",
    "    print(result)\n",
    "except:\n",
    "    print(\"Error: Denominator cannot be 0.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf40ad7",
   "metadata": {},
   "source": [
    "In the example, we are trying to divide a number by 0. Here, this code generates an exception.\n",
    "\n",
    "To handle the exception, we have put the code, result = numerator/denominator inside the try block. Now when an exception occurs, the rest of the code inside the try block is skipped.\n",
    "The except block catches the exception and statements inside the except block are executed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06008e6c",
   "metadata": {},
   "source": [
    "# 4. What are list comprehensions in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c8bfca",
   "metadata": {},
   "source": [
    "List comprehension offers a shorter syntax when you want to create a new list based on the values of an existing list.\n",
    "\n",
    "Example:\n",
    "Based on a list of fruits, if we want a new list, containing only the fruits with the letter \"a\" in the name.\n",
    "Without list comprehension we should have to write a for statement with a conditional test inside. But if we use list comprehension, it can be coded like,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31b4d2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'mango']\n"
     ]
    }
   ],
   "source": [
    "fruits = [\"apple\", \"banana\", \"cherry\", \"kiwi\", \"mango\"]\n",
    "newlist = [x for x in fruits if \"a\" in x]\n",
    "print(newlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0118eb",
   "metadata": {},
   "source": [
    "# 5. What is the purpose of the if __name__ == \"__main__\" statement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d4e538d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: It is __name__ == \"__main__\" in the question as well as in the answer. \n",
    "# Unable to edit in markdown and heading. Kindly ignore the wrong syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a82c343",
   "metadata": {},
   "source": [
    "For most practical purposes, you can think of the conditional block that you open with if __name__ == \"__main__\" as a way to store code that should only run when your file is executed as a script.\n",
    "\n",
    "If the __name__ == \"__main__\" expression is True, then the indented code following the conditional statement executes.\n",
    "If the __name__ == \"__main__\" expression is False, then Python skips the indented code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f35e5afd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the main program logic.\n",
      "This function can be used by other scripts.\n"
     ]
    }
   ],
   "source": [
    "# Example script: my_script.py\n",
    "\n",
    "def some_function():\n",
    "    print(\"This function can be used by other scripts.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # This block will only be executed if my_script.py is run directly\n",
    "    print(\"This is the main program logic.\")\n",
    "    # You can call functions, define variables, etc.\n",
    "    some_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd511ba",
   "metadata": {},
   "source": [
    "# 6. What is the purpose of the with statement in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543249f7",
   "metadata": {},
   "source": [
    "In Python, the with statement replaces a try-catch block with a concise shorthand. More importantly, it ensures closing resources right after processing them. A common example of using the with statement is reading or writing to a file. A function or class that supports the with statement is known as a context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "479d9139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with expression as variable:\n",
    "    # Code block using the acquired resource\n",
    "    # The resource is automatically acquired before entering the block\n",
    "    # and released after exiting the block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86a2bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"example.txt\", \"r\") as file:\n",
    "    #content = file.read()\n",
    "    # File is automatically closed after exiting the block"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1087d530",
   "metadata": {},
   "source": [
    "# 7. What are the key features of Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899af4f2",
   "metadata": {},
   "source": [
    "1. Fault tolerance\n",
    "2. Dynamic In Nature\n",
    "3. Lazy Evaluation\n",
    "4. Real-Time Stream Processing\n",
    "5. Speed\n",
    "6. Reusability\n",
    "7. Advanced Analytics\n",
    "8. In Memory Computing\n",
    "9. Supporting Multiple languages\n",
    "10. Integrated with Hadoop\n",
    "11. Cost efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f41f1bc",
   "metadata": {},
   "source": [
    "# 8. What are Resilient Distributed Datasets (RDDs) in Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2f7793",
   "metadata": {},
   "source": [
    "It is the actual fundamental data Structure of Apache Spark. These are immutable (Read-only) collections of objects of varying types, which computes on the different nodes of a given cluster. These provide the functionality to perform in-memory computations on large clusters in a fault-tolerant manner. Every DataSet in the Spark Resilient Distributed Dataset is well partitioned across many servers so that they can be efficiently computed on different nodes of the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "023ba59e",
   "metadata": {},
   "source": [
    "# 9. What is the difference between a DataFrame and an RDD in Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df9d688",
   "metadata": {},
   "source": [
    "DataFrame:\n",
    "DataFrame is a higher-level abstraction built on top of RDD. It represents a distributed collection of data organized into named columns, similar to a table in a relational database or a data frame in R/Python Pandas.\n",
    "\n",
    "RDD (Resilient Distributed Dataset):\n",
    "RDD is a fundamental data structure in Spark, representing an immutable distributed collection of objects that can be processed in parallel. RDDs can be created from data stored in Hadoop Distributed File System (HDFS), local file systems, or other distributed storage systems.\n",
    "\n",
    "Use Cases of RDD:\n",
    "\n",
    "1. It can easily handle data that has no predefined structure as RDD can come from any source of data.\n",
    "2. RDD is useful if we want the calculations right away.\n",
    "3. If our project is based on Java, Scala, R, and Python.\n",
    "4. If we want to specify a schema.\n",
    "5. For high-level abstraction and low-level transformation.\n",
    " \n",
    "\n",
    "Use Cases of Dataframe:\n",
    "\n",
    "1. It can handle data that come from specific sources like JSON, MySQL, CSV, etc.\n",
    "2. A dataframe is useful if we want the calculations right after the action performs.\n",
    "3. If our project is based on Java, Scala, R, and Python.\n",
    "4. If we do not want to specify a schema.\n",
    "5. For unstructured data and high-level abstractions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a96185f",
   "metadata": {},
   "source": [
    "# 10. What is Spark's ecosystem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ded7781",
   "metadata": {},
   "source": [
    "Apache Spark's ecosystem refers to a set of related projects and tools that extend and complement the core functionality of Apache Spark. The ecosystem is designed to enhance the capabilities of Spark and address various aspects of big data processing, analytics, and machine learning. Here are some key components of Spark's ecosystem:\n",
    "\n",
    "Spark SQL:\n",
    "\n",
    "Description: Spark SQL is a Spark module for structured data processing. It allows querying structured data using SQL as well as programmatically with DataFrame and Dataset APIs.\n",
    "Use Cases: Ideal for working with structured data and integrating SQL queries with Spark applications.\n",
    "\n",
    "Spark Streaming:\n",
    "\n",
    "Description: Spark Streaming is an extension of the core Spark API for processing real-time streaming data. It supports high-level operations on live data streams.\n",
    "Use Cases: Useful for real-time analytics, monitoring, and processing data from sources like Kafka, Flume, and HDFS.\n",
    "\n",
    "MLlib (Spark ML):\n",
    "\n",
    "Description: MLlib is Spark's machine learning library. It provides scalable implementations of machine learning algorithms, including classification, regression, clustering, and collaborative filtering.\n",
    "Use Cases: Applied to large-scale machine learning tasks, predictive analytics, and building machine learning pipelines.\n",
    "\n",
    "GraphX:\n",
    "\n",
    "Description: GraphX is Spark's graph processing library. It enables the efficient processing of graphs and graph-parallel computation.\n",
    "Use Cases: Useful for tasks involving graph analytics, social network analysis, and graph-based algorithms.\n",
    "\n",
    "SparkR:\n",
    "\n",
    "Description: SparkR is an R package that allows R users to interact with Spark. It provides a distributed data frame implementation and various Spark operations in R.\n",
    "Use Cases: Suitable for R users who want to leverage Spark's distributed computing capabilities.\n",
    "\n",
    "PySpark:\n",
    "\n",
    "Description: PySpark is the Python API for Spark. It allows Python developers to interface with Spark and leverage its capabilities for distributed data processing.\n",
    "Use Cases: Ideal for Python developers working on big data projects.\n",
    "\n",
    "Spark Packages:\n",
    "\n",
    "Description: Spark Packages is a community repository for sharing and discovering third-party Spark packages. These packages can extend Spark's functionality and be easily integrated into Spark applications.\n",
    "Use Cases: Developers can find and contribute packages that enhance Spark's capabilities.\n",
    "\n",
    "SparkSubmit:\n",
    "\n",
    "Description: spark-submit is a command-line tool used to submit Spark applications to a cluster. It simplifies the process of deploying and running Spark applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11394c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
